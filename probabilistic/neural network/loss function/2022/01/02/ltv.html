<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Customer Lifetime Value Prediction using a Deep Probabilistic Neural Network | mp</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Customer Lifetime Value Prediction using a Deep Probabilistic Neural Network" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A run through the paper’s neural network architecture and loss function" />
<meta property="og:description" content="A run through the paper’s neural network architecture and loss function" />
<link rel="canonical" href="https://majapavlo.github.io/blog/probabilistic/neural%20network/loss%20function/2022/01/02/ltv.html" />
<meta property="og:url" content="https://majapavlo.github.io/blog/probabilistic/neural%20network/loss%20function/2022/01/02/ltv.html" />
<meta property="og:site_name" content="mp" />
<meta property="og:image" content="https://majapavlo.github.io/blog/images/previews/ltv.PNG" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-01-02T00:00:00-06:00" />
<script type="application/ld+json">
{"datePublished":"2022-01-02T00:00:00-06:00","url":"https://majapavlo.github.io/blog/probabilistic/neural%20network/loss%20function/2022/01/02/ltv.html","@type":"BlogPosting","image":"https://majapavlo.github.io/blog/images/previews/ltv.PNG","headline":"Customer Lifetime Value Prediction using a Deep Probabilistic Neural Network","dateModified":"2022-01-02T00:00:00-06:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://majapavlo.github.io/blog/probabilistic/neural%20network/loss%20function/2022/01/02/ltv.html"},"description":"A run through the paper’s neural network architecture and loss function","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://majapavlo.github.io/blog/feed.xml" title="mp" /><link rel="shortcut icon" type="image/x-icon" href="/blog/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">mp</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">About Me</a><a class="page-link" href="/blog/_pages/blog_ref.html">Blog</a><a class="page-link" href="/blog/resources/">Resources</a><a class="page-link" href="/blog/search/">Search</a><a class="page-link" href="/blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Customer Lifetime Value Prediction using a Deep Probabilistic Neural Network</h1><p class="page-description">A run through the paper's neural network architecture and loss function</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2022-01-02T00:00:00-06:00" itemprop="datePublished">
        Jan 2, 2022
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      14 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/blog/categories/#probabilistic">probabilistic</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#neural network">neural network</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#loss function">loss function</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          
          
          
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#About">About </a></li>
<li class="toc-entry toc-h2"><a href="#Paper-Overview">Paper Overview </a></li>
<li class="toc-entry toc-h2"><a href="#Deep-Dive:-Architecture-—-Output-Layer">Deep Dive: Architecture — Output Layer </a></li>
<li class="toc-entry toc-h2"><a href="#Deep-Dive:-ZILN-Loss">Deep Dive: ZILN Loss </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Binary-Cross-Entropy-(BCE)-Loss-—-Churn">Binary Cross Entropy (BCE) Loss — Churn </a></li>
<li class="toc-entry toc-h3"><a href="#Lognormal-Loss-—-LTV">Lognormal Loss — LTV </a>
<ul>
<li class="toc-entry toc-h4"><a href="#Lognormal-takes-more-of-a-relative-look">Lognormal takes more of a relative look </a></li>
<li class="toc-entry toc-h4"><a href="#Lognormal-penalises-underestimates-more-than-overestimates">Lognormal penalises underestimates more than overestimates </a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#Zero-Inflated-Lognormal-(ZILN)-Loss-—-Churn-&-LTV-together">Zero-Inflated-Lognormal (ZILN) Loss — Churn &amp; LTV together </a></li>
<li class="toc-entry toc-h3"><a href="#Summary">Summary </a></li>
</ul>
</li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2022-01-02-ltv.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="About">
<a class="anchor" href="#About" aria-hidden="true"><span class="octicon octicon-link"></span></a>About<a class="anchor-link" href="#About"> </a>
</h2>
<p>Predicting a customer’s lifetime value (LTV) can be quite a challenging task. Wang, Liu and Miao propose using <a href="https://research.google/pubs/pub48791/">a neural network with a mixture loss</a> to handle the intricacies of churn and lifetime value modelling of new customers.</p>
<p>In this blogpost we’ll take a look at their proposed solution and go through the reasoning behind their ideas. The first section briefly summarises the importance and challenges of lifetime value modelling. It also looks at how Wang, Liu and Miao tackle these problems and quickly runs through the results of their paper. The second section then takes a deeper look at their chosen architecture, specifically the output layer. Lastly, the final section goes through the proposed loss function in more detail.</p>
<p>This post is not intended to be an in-depth mathematical explanation; but rather, is meant to provide a high-level, more approachable explanation of the paper.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Paper-Overview">
<a class="anchor" href="#Paper-Overview" aria-hidden="true"><span class="octicon octicon-link"></span></a>Paper Overview<a class="anchor-link" href="#Paper-Overview"> </a>
</h2>
<p>Why are lifetime value and churn important? Having accurate insights into customers’ future purchase behaviour can support a variety of operational and strategic business activities. For example, it can help companies with customer retention by segmenting customers according to their LTV, and targeting specific customer segments with different offers and loyalty schemes, resulting in more efficient resource spending.</p>
<p>Figure 1 shows a typical skewed LTV distribution. The large count at LTV=0 shows the huge proportion of customers who have churned (they bought a product once and then never returned). To visualise the returning customers a bit better the x-axis is displayed as the logarithm of (LTV+1). This shows that the range between returning customers’ lifetime spend varies considerably. A small proportion of high spenders can sometimes account for a large amount of business revenue. This distribution with a large proportion of one-time spenders and a few rather huge spenders poses a challenge to the traditional mean-squared-error approach. You will see this in more detail in the section that dives into their loss function below.</p>
<p><img src="/blog/images/copied_from_nb/2022-01-02-ltv/fig1.PNG" alt="" title="Figure 1"></p>
<p>Commonly, LTV and churn modelling is done separately (figure 2, left). Wang, Liu and Miao’s approach, however, allows them to tackle Churn and LTV prediction simultaneously (figure 2, right).</p>
<p><img src="/blog/images/copied_from_nb/2022-01-02-ltv/fig2.PNG" alt="" title="Figure 2"></p>
<p>Instead of looking at the LTV and Churn distributions separately, they propose using “a mixture of zero-point mass and a lognormal distribution”, calling it the zero-inflated lognormal distribution (ZILN). They then derive a mixture loss by taking the negative log-likelihood of that ZILN distribution and then use it to train their neural network on both tasks simultaneously. Too much information? Don’t worry about it. We will look at what loss functions are and how the ZILN works in more detail below. What you should take away for now is that you can use one model to predict the two tasks: Churn and LTV.</p>
<p>Wang, Liu and Miao measure the performance of their approach on the two subtasks. While the Churn prediction achieves a comparable performance with a classical binary classification loss, their LTV prediction task outperforms the traditional mean-squared-error (MSE) approach on three different evaluation metrics. We won’t dissect their evaluation metrics in this post, but you can take a look at their paper if you’d like to understand their results more.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Deep-Dive:-Architecture-—-Output-Layer">
<a class="anchor" href="#Deep-Dive:-Architecture-%E2%80%94-Output-Layer" aria-hidden="true"><span class="octicon octicon-link"></span></a>Deep Dive: Architecture — Output Layer<a class="anchor-link" href="#Deep-Dive:-Architecture-%E2%80%94-Output-Layer"> </a>
</h2>
<p>As mentioned above Wang, Liu and Miao use a neural network for their model. They opt for a simple feed-forward neural network with 64 units on the first hidden layer and 32 on the second. See the figure 3 below. These hidden units use the ReLu activation function. Numerical inputs are fed in directly, while categorical inputs are encoded as embeddings. We will focus on the output layer as these units are interesting to the loss function that we will discuss below.</p>
<p><img src="/blog/images/copied_from_nb/2022-01-02-ltv/fig3.PNG" alt="" title="Figure 3"></p>
<p>The output layer has one unit <strong>p</strong> to predict the probability of someone having churned or not. The activation function used to represent that probability is a sigmoid function. This function works well for outputting probabilities as its range of output values is between 0 and 1. A typical threshold is 0.5. A prediction that is below 0.5 will be a customer who’s still alive, while those with a prediction above 0.5 will be predicted as churned. See figure 4 below.</p>
<p><img src="/blog/images/copied_from_nb/2022-01-02-ltv/fig4.PNG" alt="" title="Figure 4"></p>
<p>LTV on the other hand needs two units. One unit to predict a <a href="https://en.wikipedia.org/wiki/Location_parameter"><strong>location parameter μ</strong></a> and another unit to predict the <a href="https://en.wikipedia.org/wiki/Scale_parameter"><strong>scale parameter σ</strong></a>. We then use these two parameters to define a full prediction distribution, which in turn provides us with an uncertainty measure connected to our prediction (the uncertainty can be estimated by using the quantiles of our predicted distribution). These two outputs μ and σ <strong>should not be mistaken with the more familiar mean and standard</strong> deviation from a normal distribution.</p>
<p>The μ unit has no constraints in terms of range or sign. Thus, Wang, Liu and Miao opted to use an identity function for μ. This means it simply outputs the predicted value of the neural networks μ unit without running it through an activation function.</p>
<p>The scale parameter σ should only be able to return positive outputs. A common function like the exponential can solve this positivity constraint, but due to its steep growth can easily result in <a href="https://www.analyticsvidhya.com/blog/2021/06/the-challenge-of-vanishing-exploding-gradients-in-deep-neural-networks/">exploding gradients</a>. Wang, Liu and Miao opted for a softplus activation function which doesn’t grow as steeply as the exponential function. However, the softplus alone didn’t seem enough to avoid this instability when training their model so they also applied <a href="https://towardsdatascience.com/what-is-gradient-clipping-b8e815cdfb48">gradient clipping</a>.</p>
<p><img src="/blog/images/copied_from_nb/2022-01-02-ltv/fig5.PNG" alt="" title="Figure 5"></p>
<p>The output layer then ends up looking like figure 6 below. <strong>Churn</strong> just needs one probability value <strong>p</strong>, whereas <strong>LTV</strong> has both <strong>μ and σ</strong>. By learning to represent multiple tasks (both Churn and LTV) using one model, the middle layers of this network learn to generalise better on each subtask. If you want to understand more about this, have a read on multi-task learning <a href="https://ruder.io/multi-task/">here</a>.</p>
<p><img src="/blog/images/copied_from_nb/2022-01-02-ltv/fig6.PNG" alt="" title="Figure 6"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Deep-Dive:-ZILN-Loss">
<a class="anchor" href="#Deep-Dive:-ZILN-Loss" aria-hidden="true"><span class="octicon octicon-link"></span></a>Deep Dive: ZILN Loss<a class="anchor-link" href="#Deep-Dive:-ZILN-Loss"> </a>
</h2>
<p>To train this network we minimise the error between the neural network’s predictions and the actual target value from the data. In our example this means we compare our predicted churn probability to the actual churn value. This performance measure is called the loss.</p>
<p>Depending on what you are trying to achieve, you use a different loss function that you optimise your neural network against. Given that the architecture above combines two tasks we will need a loss function that is tailored to each subtask.</p>
<p>Two common loss functions are Binary Cross Entropy (BCE) loss and Mean Squared Error (MSE) loss. BCE is used for binary classification tasks where you are trying to predict if something is true or false; meaning there are only two states. In our case that would be whether someone has churned or not. Then there is MSE which is a commonly used loss for regression problems where you are trying to predict a continuous value such as a salary, stocks or in our case LTV.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Binary-Cross-Entropy-(BCE)-Loss-—-Churn">
<a class="anchor" href="#Binary-Cross-Entropy-(BCE)-Loss-%E2%80%94-Churn" aria-hidden="true"><span class="octicon octicon-link"></span></a>Binary Cross Entropy (BCE) Loss — Churn<a class="anchor-link" href="#Binary-Cross-Entropy-(BCE)-Loss-%E2%80%94-Churn"> </a>
</h3>
<p>For the churn task of the neural network Wang, Liu and Miao use the BCE loss:</p>
<p>
$$BCE  Loss = -\frac{1}{N} \sum_{i=1}^{N} -(y_i*log(p_i) + (1-y_i) * log(1-p_i))$$

        yᵢ is the true target value (1 for churn and 0 for no churn), pᵢ is the predicted probability of yᵢ =churn while (1-pᵢ) is the exact opposite (no churn)</p>
<p>The aim in our case is to correctly classify churn (class 1) and no churn (class 0). We need a way to handle both classes in one function and put a higher penalty on wrong predictions. This is achieved by activating only one part of the loss function, while keeping the other part deactivated. Giving wrong predictions a much higher error is achieved by using the logarithm. Let’s look at an example to clarify this.</p>
<p>To keep it simple, we will just look at one observation where we have the true observation belonging to class 1 (churn, y₁=1), this removes the summation, and we are left with:</p>
<p>
$$error_1 = -(y_i*log(p_i) + (1-y_i) * log(1-p_i))$$
</p>
<p>Now let’s say our model predicts this observation to be churn with a probability of 0.95 (p₁=0.95). That means that we get the following error for our observation:</p>
<p>
$$error_1 = -(1*log(0.95) + (1-1) * log(1-0.95))$$


$$        = -(1*log(0.95) + 0 * log(0.05))$$


$$        = -(1*log(0.95))$$


$$        \approx 0.051 $$
</p>
<p>This shows that only the first part of the equation is activated, the other part (multiplying by 0) cancels out. So, we are left with the log() of our probability which gives us tiny loss values when pᵢ is close to 1 and gets larger and larger the closer our probability gets to the value of 0 (as in us predicting the wrong class), see figure 8 (left) below. In our example we have a probability of 0.95 and therefore end up with a small error value of 0.051 for our observation. If instead, our true value is now 0 instead of 1, then the other part of the loss function activates, and we end up having a large error of 2.966 if we are still predicting that it is churn (=1) with a probability of 0.95, see figure 9 (right) below.</p>
<p><img src="/blog/images/copied_from_nb/2022-01-02-ltv/fig7.png" alt="" title="Figure 7"></p>
<p>This example is summarised in the first 2 rows of table 1 below. The table, additionally, shows how the loss function behaves in the opposite scenario where the predicted value is 0 with pᵢ=0.01.</p>
<p><img src="/blog/images/copied_from_nb/2022-01-02-ltv/table1.PNG" alt="" title="Table 1"></p>
<p>This example should give you a general idea why BCE works well for tasks where you are predicting either one class or the other. Having such a large error on wrong predictions will force the model to learn a better representation which should minimise the overall error in your model’s prediction performance.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Lognormal-Loss-—-LTV">
<a class="anchor" href="#Lognormal-Loss-%E2%80%94-LTV" aria-hidden="true"><span class="octicon octicon-link"></span></a>Lognormal Loss — LTV<a class="anchor-link" href="#Lognormal-Loss-%E2%80%94-LTV"> </a>
</h3>
<p>As mentioned above, rather than predicting a single point estimate for the LTV task of the network it is preferable to have a full prediction distribution. The distribution can give us an idea about the spread of observation points around our predicted LTV value. Wang, Liu and Miao propose using the lognormal distribution. The two parameters μ and σ from above describe the distribution’s probability density function (PDF): $ PDF = \frac{1}{x \sigma \sqrt{2 \pi}} \exp \left( - \frac{(\log x - \mu)^2}{2 \sigma^2} \right) $ See figure 8 below. This lognormal distribution resembles typical LTV data quite well: it is right-skewed, starting at zero and going into positive infinity. Hence, it makes sense for the neural network to learn the parameters of the lognormal distribution.</p>
<p><img src="/blog/images/copied_from_nb/2022-01-02-ltv/fig8.png" alt="" title="Figure 8"></p>
<p>We can obtain the lognormal loss by taking the negative log-likelihood of the lognormal distribution, see the loss function below. We won’t go through the steps of deriving the loss. The main idea is to simply compare the lognormal loss to the classic MSE loss and see how it behaves differently to the MSE.</p>
<p>
$$ Lognormal  Loss = \frac{1}{N} \sum_{i=1}^{N} \log(x_i \sigma \sqrt{2 \pi}) + \frac{(\log x_i - \mu)^2}{2 \sigma^2}$$

        xᵢ is our actual true value, μ &amp; σ are the estimated parameters for the lognormal probability distribution, N is the number of samples</p>
<p>Versus the MSE loss:

$$ MSE  Loss = \frac{1}{N} \sum_{i=1}^{N} (x_i - \hat{x_i})^2 $$

        xᵢ is our actual true value and xᵢ is the predicted value, N is the number of samples</p>
<p>Let’s look at a few examples. The first example will show us how these losses differ on small and large prediction values. The second example will show us how they differ when predictions are over- or under-estimated.</p>
<h4 id="Lognormal-takes-more-of-a-relative-look">
<a class="anchor" href="#Lognormal-takes-more-of-a-relative-look" aria-hidden="true"><span class="octicon octicon-link"></span></a>Lognormal takes more of a relative look<a class="anchor-link" href="#Lognormal-takes-more-of-a-relative-look"> </a>
</h4>
<p>We have a case where our true median value of the lognormal distribution is 20 and another case where this true value is 20,000. The table below compares the MSE loss to the Lognormal loss (with two different σ² values). We can see that the <strong>MSE penalises high spenders</strong> (case 2, loss: 16,000,000) <strong>more harshly</strong> compared to lower spenders (case 1, loss: 16). The loss in case 2 is 1,000,000 times larger than for case 1, despite the relative deviation from the true value being the same.</p>
<p><img src="/blog/images/copied_from_nb/2022-01-02-ltv/table2.PNG" alt="" title="Table 2"></p>
<p>Given that a high-value customer typically spends orders of magnitude more than a normal customer, the MSE is less suited for this problem. It would over-penalise the prediction errors for our high value customers. The Lognormal on the other hand will treat small differences from the average spending customer almost the same as large differences from high-value customer predictions.</p>
<h4 id="Lognormal-penalises-underestimates-more-than-overestimates">
<a class="anchor" href="#Lognormal-penalises-underestimates-more-than-overestimates" aria-hidden="true"><span class="octicon octicon-link"></span></a>Lognormal penalises underestimates more than overestimates<a class="anchor-link" href="#Lognormal-penalises-underestimates-more-than-overestimates"> </a>
</h4>
<p>Here both cases have a true value of 20, but in case 1 we under-predict by 6, whereas in case 2 we over-predict by 6. The table below compares the different losses on both cases again. We can see that the MSE penalises underestimates and overestimates equally, while the lognormal loss penalises underestimates more than overestimates.</p>
<p><img src="/blog/images/copied_from_nb/2022-01-02-ltv/table3.PNG" alt="" title="Table 3"></p>
<p>This asymmetric behaviour is visualised in the figure 9 below<sup id="fnref-1" class="footnote-ref"><a href="#fn-1">1</a></sup>. While the MSE is symmetrical around the minimum, it shows that the lognormal loss gets more asymmetric as σ² increases.</p>
<p><img src="/blog/images/copied_from_nb/2022-01-02-ltv/fig9.png" alt="" title="Figure 9"></p>
<p>So far, we’ve looked at the loss function in terms of its median. The median is defined as exp(μ), meaning we used μ=log(x) in the loss function. Wang, Liu and Miao, however, suggest predicting LTV as the mean of the lognormal distribution, which is:</p>
<p>
$$ E(X) = \exp \left(\mu + \frac{\sigma^2}{2} \right)$$
</p>
<p>This means we substitute μ with $\log(x) - \frac{σ²}{2}$ in the loss function. The loss still behaves in the same way as above. The only difference this creates is a bias in our predictions towards a higher number. The higher σ² the higher our predicted LTV will be. If for instance our true value is 20, and we perfectly predict the μ-unit but have a σ² of 0.3, then our predicted LTV will be about 23.2. The same value with a σ² of only 0.01 would give us an LTV prediction of about 20.1 (much closer to our true value). We can see the shift in minimum prediction in the figure 10 below<sup id="fnref-2" class="footnote-ref"><a href="#fn-2">2</a></sup>.</p>
<p><img src="/blog/images/copied_from_nb/2022-01-02-ltv/fig10.png" alt="" title="Figure 10"></p>
<p>Both the mean and median are viable approaches when using the lognormal. But when do we use the lognormal instead of the MSE? The lognormal loss should be considered more favourable to the MSE when the range of our true values (LTV) is large as we don’t want large values to be penalised more.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Zero-Inflated-Lognormal-(ZILN)-Loss-—-Churn-&amp;-LTV-together">
<a class="anchor" href="#Zero-Inflated-Lognormal-(ZILN)-Loss-%E2%80%94-Churn-&amp;-LTV-together" aria-hidden="true"><span class="octicon octicon-link"></span></a>Zero-Inflated-Lognormal (ZILN) Loss — Churn &amp; LTV together<a class="anchor-link" href="#Zero-Inflated-Lognormal-(ZILN)-Loss-%E2%80%94-Churn-&amp;-LTV-together"> </a>
</h3>
<p>Now that we’ve looked at the different tasks individually the final step to getting the ZILN mixture loss is straight-forward. It is simply a linear combination of the two losses from above. See figure 11 below.</p>
<p><img src="/blog/images/copied_from_nb/2022-01-02-ltv/fig11.PNG" alt="" title="Figure 11"></p>
<p>Wang, Liu and Miao assume an equal weighting for each subtask, so the loss function ends up looking as follows:</p>
<p>
$$ ZILN Loss = MSE Loss + Lognormal Loss$$
</p>
<p>This means we end up training the neural network by simply minimising the sum of the two loss functions (BCE and Lognormal loss).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Summary">
<a class="anchor" href="#Summary" aria-hidden="true"><span class="octicon octicon-link"></span></a>Summary<a class="anchor-link" href="#Summary"> </a>
</h3>
<p>In this blog post you had a look at the paper: <em>A Deep Probabilistic Model for Customer Lifetime Value Prediction</em>, which proposes a multi-task approach to LTV modelling. You explored their proposed output units and the associated BCE and lognormal loss functions that when combined, result in their proposed ZILN loss. The benefits of this approach are:</p>
<ul>
<li>Having an LTV prediction distribution instead of a single point prediction (which gives you insights into the uncertainty of your predictions)</li>
<li>Better generalisation on both churn and LTV modelling, due to the multitask learning approach</li>
<li>Reduction of the engineering effort around, building, training and maintaining two separate models</li>
</ul>
<p>What this post didn’t cover is how the results compare to other models or how to apply this model to a use case. Given that their <a href="https://github.com/google/lifetime_value">code</a> is available online you should be able to use it as is for modelling new customers’ expectations or adapt it to model the lifetime over time <strong>using a sequential neural network architecture such as an RNN or LSTM.</strong></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><em>Footnotes</em></p>
<p></p>
<div class="footnotes"><p id="fn-1">1. For all losses to start at the same minimum y-value the minimum of each loss itself was subtracted, which is why the y-axis values differ to the direct loss calculations from the table above.<a href="#fnref-1" class="footnote footnotes">↩</a></p></div>
<p></p>
<div class="footnotes"><p id="fn-2">2. The figure in the paper looks like it has been scaled on each of the loss functions to show the effect of shift in a clearer way<a href="#fnref-2" class="footnote footnotes">↩</a></p></div>

</div>
</div>
</div>
</div>



  </div><a class="u-url" href="/blog/probabilistic/neural%20network/loss%20function/2022/01/02/ltv.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Applied deep learning and other machine learning topics</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/majapavlo" target="_blank" title="majapavlo"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://www.linkedin.com/in/https%3A%2F%2Fwww.linkedin.com%2Fin%2Fmaja-pavlovic%2F" target="_blank" title="https://www.linkedin.com/in/maja-pavlovic/"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#linkedin"></use></svg></a></li><li><a rel="me" href="https://twitter.com/maja_pavlo" target="_blank" title="maja_pavlo"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
