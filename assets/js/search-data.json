{
  
    
        "post0": {
            "title": "Customer Lifetime Value Prediction using a Deep Probabilistic Neural Network",
            "content": "About . Predicting a customer’s lifetime value (LTV) can be quite a challenging task. Wang, Liu and Miao propose using a neural network with a mixture loss to handle the intricacies of churn and lifetime value modelling of new customers. . In this blogpost we’ll take a look at their proposed solution and go through the reasoning behind their ideas. The first section briefly summarises the importance and challenges of lifetime value modelling. It also looks at how Wang, Liu and Miao tackle these problems and quickly runs through the results of their paper. The second section then takes a deeper look at their chosen architecture, specifically the output layer. Lastly, the final section goes through the proposed loss function in more detail. . This post is not intended to be an in-depth mathematical explanation; but rather, is meant to provide a high-level, more approachable explanation of the paper. . Paper Overview . Why are lifetime value and churn important? Having accurate insights into customers’ future purchase behaviour can support a variety of operational and strategic business activities. For example, it can help companies with customer retention by segmenting customers according to their LTV, and targeting specific customer segments with different offers and loyalty schemes, resulting in more efficient resource spending. . Figure 1 shows a typical skewed LTV distribution. The large count at LTV=0 shows the huge proportion of customers who have churned (they bought a product once and then never returned). To visualise the returning customers a bit better the x-axis is displayed as the logarithm of (LTV+1). This shows that the range between returning customers’ lifetime spend varies considerably. A small proportion of high spenders can sometimes account for a large amount of business revenue. This distribution with a large proportion of one-time spenders and a few rather huge spenders poses a challenge to the traditional mean-squared-error approach. You will see this in more detail in the section that dives into their loss function below. . - Picute . Commonly, LTV and churn modelling is done separately (figure 2, left). Wang, Liu and Miao’s approach, however, allows them to tackle Churn and LTV prediction simultaneously (figure 2, right). . Instead of looking at the LTV and Churn distributions separately, they propose using “a mixture of zero-point mass and a lognormal distribution”, calling it the zero-inflated lognormal distribution (ZILN). They then derive a mixture loss by taking the negative log-likelihood of that ZILN distribution and then use it to train their neural network on both tasks simultaneously. Too much information? Don’t worry about it. We will look at what loss functions are and how the ZILN works in more detail below. What you should take away for now is that you can use one model to predict the two tasks: Churn and LTV. . Wang, Liu and Miao measure the performance of their approach on the two subtasks. While the Churn prediction achieves a comparable performance with a classical binary classification loss, their LTV prediction task outperforms the traditional mean-squared-error (MSE) approach on three different evaluation metrics. We won’t dissect their evaluation metrics in this post, but you can take a look at their paper if you’d like to understand their results more. .",
            "url": "https://majapavlo.github.io/blog/jupyter/2021/01/02/ltv.html",
            "relUrl": "/jupyter/2021/01/02/ltv.html",
            "date": " • Jan 2, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "test .",
            "url": "https://majapavlo.github.io/blog/jupyter/2021/01/01/test.html",
            "relUrl": "/jupyter/2021/01/01/test.html",
            "date": " • Jan 1, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://majapavlo.github.io/blog/2020/01/14/test-markdown-post.html",
            "relUrl": "/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  

  

  

  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page13": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://majapavlo.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}